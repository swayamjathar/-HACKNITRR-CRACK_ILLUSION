{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a58268c",
   "metadata": {},
   "source": [
    "# वरदान\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62403405",
   "metadata": {},
   "source": [
    "Team: CRACK_ILLUSION (Swayam Jathar & Erum Noor)\n",
    "\n",
    "\n",
    "There are approximately 66 million people in India suffering from either complete or partial deafness, and at least 50lakh are children. These people are specially abled and not able to communicate with other beings, easily. To communicate with others they use sign language, which becomes tough for others to understand it. So circumscribing the this major issue we decided to simplfy the mode of communication between specially abled people and normal people. Introducing \"वरदान\" which works as a trasnlator from Sign Language to Text. The motto of \"वरदान\" is o create a meaningful connection with specially abled people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c934a",
   "metadata": {},
   "source": [
    "Instaltalling the prerequisite librabries. Further libraries will install when we need it in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668a735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[0.2624, 0.2967, 0.3138,  ..., 0.7248, 0.7077, 0.7077],\n",
      "          [0.2796, 0.3138, 0.3309,  ..., 0.7419, 0.7248, 0.7248],\n",
      "          [0.2967, 0.3138, 0.3481,  ..., 0.7762, 0.7591, 0.7419],\n",
      "          ...,\n",
      "          [0.5022, 0.5364, 0.6049,  ..., 1.1872, 1.1872, 1.1872],\n",
      "          [0.4851, 0.5193, 0.6049,  ..., 1.2043, 1.1872, 1.1872],\n",
      "          [0.4679, 0.5193, 0.6049,  ..., 1.2043, 1.1872, 1.1872]]],\n",
      "\n",
      "\n",
      "        [[[0.9646, 1.0159, 1.0844,  ..., 0.9988, 0.9817, 0.9303],\n",
      "          [0.9817, 1.0331, 1.0844,  ..., 1.0331, 0.9988, 0.9646],\n",
      "          [0.9988, 1.0502, 1.1015,  ..., 1.0502, 1.0331, 0.9817],\n",
      "          ...,\n",
      "          [1.4440, 1.5125, 1.5468,  ..., 1.4954, 1.4612, 1.4269],\n",
      "          [1.4440, 1.4954, 1.5297,  ..., 1.4954, 1.4783, 1.4440],\n",
      "          [1.3927, 1.4440, 1.5810,  ..., 1.4783, 1.4612, 1.4098]]]]), 'label': tensor([[16.],\n",
      "        [19.]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "class SignLanguageMNIST(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_mapping():\n",
    "  \n",
    "        mapping = list(range(25))\n",
    "        mapping.pop(9)\n",
    "        return mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def read_label_samples_from_csv(path: str):\n",
    "        \n",
    "        mapping = SignLanguageMNIST.get_label_mapping()\n",
    "        labels, samples = [], []\n",
    "        with open(path) as f:\n",
    "            _ = next(f)  # skip header\n",
    "            for line in csv.reader(f):\n",
    "                label = int(line[0])\n",
    "                labels.append(mapping.index(label))\n",
    "                samples.append(list(map(int, line[1:])))\n",
    "        return labels, samples\n",
    "\n",
    "    def __init__(self,\n",
    "            path: str=\"sign_mnist_train.csv\",\n",
    "            mean: List[float]=[0.485],\n",
    "            std: List[float]=[0.229]):\n",
    "      \n",
    "        labels, samples = SignLanguageMNIST.read_label_samples_from_csv(path)\n",
    "        self._samples = np.array(samples, dtype=np.uint8).reshape((-1, 28, 28, 1))\n",
    "        self._labels = np.array(labels, dtype=np.uint8).reshape((-1, 1))\n",
    "\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(28, scale=(0.8, 1.2)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self._mean, std=self._std)])\n",
    "\n",
    "        return {\n",
    "            'image': transform(self._samples[idx]).float(),\n",
    "            'label': torch.from_numpy(self._labels[idx]).float()\n",
    "        }\n",
    "\n",
    "\n",
    "def get_train_test_loaders(batch_size=32):\n",
    "    trainset = SignLanguageMNIST('sign_mnist_train.csv')\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    testset = SignLanguageMNIST('sign_mnist_test.csv')\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loader, _ = get_train_test_loaders(2)\n",
    "    print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb620639",
   "metadata": {},
   "source": [
    "We have used a technique called data augmentation, where samples are disturbed during training, to increase the model’s robustness to these perturbations. In particular, randomly zoom in on the image by varying amounts and on different locations, via RandomResizedCrop. To normalize the inputs so that image values are rescaled to the [0, 1] range in expectation, instead of [0, 255]; to accomplish this, we used the dataset _mean and _std when normalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91bff6",
   "metadata": {},
   "source": [
    "Dataset is being taken from Kaggle: https://www.kaggle.com/datamunge/sign-language-mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ac25fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,     0] loss: 3.178262\n",
      "[0,   100] loss: 3.177437\n",
      "[0,   200] loss: 3.175903\n",
      "[0,   300] loss: 3.170432\n",
      "[0,   400] loss: 3.076334\n",
      "[0,   500] loss: 2.852937\n",
      "[0,   600] loss: 2.605645\n",
      "[0,   700] loss: 2.378769\n",
      "[0,   800] loss: 2.176646\n",
      "[1,     0] loss: 0.429329\n",
      "[1,   100] loss: 0.530480\n",
      "[1,   200] loss: 0.480089\n",
      "[1,   300] loss: 0.446717\n",
      "[1,   400] loss: 0.410137\n",
      "[1,   500] loss: 0.387869\n",
      "[1,   600] loss: 0.365638\n",
      "[1,   700] loss: 0.338897\n",
      "[1,   800] loss: 0.324282\n",
      "[2,     0] loss: 0.270173\n",
      "[2,   100] loss: 0.186629\n",
      "[2,   200] loss: 0.180923\n",
      "[2,   300] loss: 0.160046\n",
      "[2,   400] loss: 0.157416\n",
      "[2,   500] loss: 0.152737\n",
      "[2,   600] loss: 0.150285\n",
      "[2,   700] loss: 0.142306\n",
      "[2,   800] loss: 0.137463\n",
      "[3,     0] loss: 0.066076\n",
      "[3,   100] loss: 0.113861\n",
      "[3,   200] loss: 0.112179\n",
      "[3,   300] loss: 0.110203\n",
      "[3,   400] loss: 0.105925\n",
      "[3,   500] loss: 0.102603\n",
      "[3,   600] loss: 0.096586\n",
      "[3,   700] loss: 0.094647\n",
      "[3,   800] loss: 0.092531\n",
      "[4,     0] loss: 0.018499\n",
      "[4,   100] loss: 0.056363\n",
      "[4,   200] loss: 0.060476\n",
      "[4,   300] loss: 0.065242\n",
      "[4,   400] loss: 0.067779\n",
      "[4,   500] loss: 0.064279\n",
      "[4,   600] loss: 0.062913\n",
      "[4,   700] loss: 0.064217\n",
      "[4,   800] loss: 0.065971\n",
      "[5,     0] loss: 0.000741\n",
      "[5,   100] loss: 0.059299\n",
      "[5,   200] loss: 0.064662\n",
      "[5,   300] loss: 0.065146\n",
      "[5,   400] loss: 0.064791\n",
      "[5,   500] loss: 0.062951\n",
      "[5,   600] loss: 0.059424\n",
      "[5,   700] loss: 0.059147\n",
      "[5,   800] loss: 0.058696\n",
      "[6,     0] loss: 0.057589\n",
      "[6,   100] loss: 0.048213\n",
      "[6,   200] loss: 0.051506\n",
      "[6,   300] loss: 0.053035\n",
      "[6,   400] loss: 0.052011\n",
      "[6,   500] loss: 0.046510\n",
      "[6,   600] loss: 0.047127\n",
      "[6,   700] loss: 0.046848\n",
      "[6,   800] loss: 0.045142\n",
      "[7,     0] loss: 0.004775\n",
      "[7,   100] loss: 0.044773\n",
      "[7,   200] loss: 0.047098\n",
      "[7,   300] loss: 0.040550\n",
      "[7,   400] loss: 0.042280\n",
      "[7,   500] loss: 0.041242\n",
      "[7,   600] loss: 0.040446\n",
      "[7,   700] loss: 0.043166\n",
      "[7,   800] loss: 0.044248\n",
      "[8,     0] loss: 0.020864\n",
      "[8,   100] loss: 0.056481\n",
      "[8,   200] loss: 0.052310\n",
      "[8,   300] loss: 0.050167\n",
      "[8,   400] loss: 0.042269\n",
      "[8,   500] loss: 0.040097\n",
      "[8,   600] loss: 0.036710\n",
      "[8,   700] loss: 0.037115\n",
      "[8,   800] loss: 0.036424\n",
      "[9,     0] loss: 0.000550\n",
      "[9,   100] loss: 0.012743\n",
      "[9,   200] loss: 0.020185\n",
      "[9,   300] loss: 0.025246\n",
      "[9,   400] loss: 0.028432\n",
      "[9,   500] loss: 0.028700\n",
      "[9,   600] loss: 0.028248\n",
      "[9,   700] loss: 0.031526\n",
      "[9,   800] loss: 0.031010\n",
      "[10,     0] loss: 0.005864\n",
      "[10,   100] loss: 0.027070\n",
      "[10,   200] loss: 0.021158\n",
      "[10,   300] loss: 0.019990\n",
      "[10,   400] loss: 0.017452\n",
      "[10,   500] loss: 0.015700\n",
      "[10,   600] loss: 0.014114\n",
      "[10,   700] loss: 0.013042\n",
      "[10,   800] loss: 0.013121\n",
      "[11,     0] loss: 0.030141\n",
      "[11,   100] loss: 0.008802\n",
      "[11,   200] loss: 0.009529\n",
      "[11,   300] loss: 0.008918\n",
      "[11,   400] loss: 0.008111\n",
      "[11,   500] loss: 0.007986\n",
      "[11,   600] loss: 0.007833\n",
      "[11,   700] loss: 0.007639\n",
      "[11,   800] loss: 0.008506\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 3)\n",
    "        self.conv3 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 48)\n",
    "        self.fc3 = nn.Linear(48, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    net = Net().float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    trainloader, _ = get_train_test_loaders()\n",
    "    for epoch in range(12):  # loop over the dataset multiple times\n",
    "        train(net, criterion, optimizer, trainloader, epoch)\n",
    "        scheduler.step()\n",
    "    torch.save(net.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "\n",
    "def train(net, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = Variable(data['image'].float())\n",
    "        labels = Variable(data['label'].long())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch, i, running_loss / (i + 1)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e96da",
   "metadata": {},
   "source": [
    "We see a neral network train as output of the above code. Hence we have deployed a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1dd453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PyTorch ==========\n",
      "Training accuracy: 99.8\n",
      "Validation accuracy: 97.5\n",
      "========== ONNX ==========\n",
      "Training accuracy: 99.8\n",
      "Validation accuracy: 97.4\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    \"\"\"Evaluate neural network outputs against non-one-hotted labels.\"\"\"\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))\n",
    "\n",
    "\n",
    "def batch_evaluate(\n",
    "        net: Net,\n",
    "        dataloader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"Evaluate neural network in batches, if dataset is too large.\"\"\"\n",
    "    score = n = 0.0\n",
    "    for batch in dataloader:\n",
    "        n += len(batch['image'])\n",
    "        outputs = net(batch['image'])\n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            outputs = outputs.detach().numpy()\n",
    "        score += evaluate(outputs, batch['label'][:, 0])\n",
    "    return score / n\n",
    "\n",
    "\n",
    "def validate():\n",
    "    trainloader, testloader = get_train_test_loaders()\n",
    "    net = Net().float().eval()\n",
    "\n",
    "    pretrained_model = torch.load(\"checkpoint.pth\")\n",
    "    net.load_state_dict(pretrained_model)\n",
    "\n",
    "    print('=' * 10, 'PyTorch', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "    trainloader, testloader = get_train_test_loaders(1)\n",
    "\n",
    "    # export to onnx\n",
    "    fname = \"signlanguage.onnx\"\n",
    "    dummy = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(net, dummy, fname, input_names=['input'])\n",
    "\n",
    "    # check exported model\n",
    "    model = onnx.load(fname)\n",
    "    onnx.checker.check_model(model)  # check model is well-formed\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_session = ort.InferenceSession(fname)\n",
    "    net = lambda inp: ort_session.run(None, {'input': inp.data.numpy()})[0]\n",
    "\n",
    "    print('=' * 10, 'ONNX', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6002c",
   "metadata": {},
   "source": [
    "We obtain following accuracy for training and validation dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9864edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def center_crop(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    start = abs(h - w) // 2\n",
    "    if h > w:\n",
    "        return frame[start: start + w]\n",
    "    return frame[:, start: start + h]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # constants\n",
    "    index_to_letter = list('ABCDEFGHIKLMNOPQRSTUVWXY')\n",
    "    mean = 0.485 * 255.\n",
    "    std = 0.229 * 255.\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_session = ort.InferenceSession(\"signlanguage.onnx\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # preprocess data\n",
    "        frame = center_crop(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        x = cv2.resize(frame, (28, 28))\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        x = x.reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        y = ort_session.run(None, {'input': x})[0]\n",
    "\n",
    "        index = np.argmax(y, axis=1)\n",
    "        letter = index_to_letter[int(index)]\n",
    "\n",
    "        cv2.putText(frame, letter, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), thickness=2)\n",
    "        cv2.imshow(\"Sign Language Translator\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a16879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
